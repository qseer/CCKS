{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec4153fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Volumes/学习/CCKS/CCKS_base.ipynb Cell 1'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Volumes/%E5%AD%A6%E4%B9%A0/CCKS/CCKS_base.ipynb#ch0000000?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Volumes/%E5%AD%A6%E4%B9%A0/CCKS/CCKS_base.ipynb#ch0000000?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjson\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Volumes/%E5%AD%A6%E4%B9%A0/CCKS/CCKS_base.ipynb#ch0000000?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcv2\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mcv\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Volumes/%E5%AD%A6%E4%B9%A0/CCKS/CCKS_base.ipynb#ch0000000?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mPIL\u001b[39;00m \u001b[39mimport\u001b[39;00m Image\n\u001b[1;32m      <a href='vscode-notebook-cell:/Volumes/%E5%AD%A6%E4%B9%A0/CCKS/CCKS_base.ipynb#ch0000000?line=4'>5</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import cv2 as cv\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.utils.data as Data\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c9948b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\"1868\": {\"question\": \"\", \\n    \"type\": \"TF\", \\n    \"difficulty\": \"easy\", \\n    \"diagram_path\": \"./data_re_split\\\\data_CCKS22\\\\CSDia_train_val_test\\\\train\\\\D\\\\958.png\", \\n    \"cords\": [[36, 134, 68, 65], [102, 134, 65, 65], [164, 133, 64, 67], [227, 134, 64, 65], [290, 134, 63, 64], [351, 134, 66, 64], [60, 87, 22, 42], [312, 87, 23, 43]], \\n    \"answer\": {\"a\": \"True\", \"b\": \"False\"}, \\n    \"correct_answer\": \"b\", \\n    \"split\": \"train\"},\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#####\n",
    "'''\n",
    "\"1868\": {\"question\": \"\", \n",
    "    \"type\": \"TF\", \n",
    "    \"difficulty\": \"easy\", \n",
    "    \"diagram_path\": \"./data_re_split\\\\data_CCKS22\\\\CSDia_train_val_test\\\\train\\\\D\\\\958.png\", \n",
    "    \"cords\": [[36, 134, 68, 65], [102, 134, 65, 65], [164, 133, 64, 67], [227, 134, 64, 65], [290, 134, 63, 64], [351, 134, 66, 64], [60, 87, 22, 42], [312, 87, 23, 43]], \n",
    "    \"answer\": {\"a\": \"True\", \"b\": \"False\"}, \n",
    "    \"correct_answer\": \"b\", \n",
    "    \"split\": \"train\"},\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bbd76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(Q_path):\n",
    "    with open(Q_path,'r') as f:\n",
    "        for line in f:\n",
    "            data_json = json.loads(line)\n",
    "    MC_diagram_path_s = []\n",
    "    MC_question_s = []\n",
    "    MC_type_s = []\n",
    "    MC_answer_s = []\n",
    "    MC_correct_answer_s = []\n",
    "    MC_cords_s = []\n",
    "    \n",
    "    TF_diagram_path_s = []\n",
    "    TF_question_s = []\n",
    "    TF_type_s = []\n",
    "    TF_answer_s = []\n",
    "    TF_correct_answer_s = []\n",
    "    TF_cords_s = []\n",
    "    \n",
    "    for i in range(len(data_json)):\n",
    "        #print('i', i, str(i))\n",
    "        if data_json[str(i+1)]['question'] != '':\n",
    "            if data_json[str(i+1)]['type'] == 'MC':\n",
    "                MC_diagram_path_s.append(data_json[str(i+1)]['diagram_path'].split(\"\\\\\")[-1])\n",
    "                MC_question_s.append(data_json[str(i+1)]['question'])\n",
    "                MC_type_s.append(data_json[str(i+1)]['type'])\n",
    "                MC_answer_s.append(data_json[str(i+1)]['answer'])\n",
    "                MC_correct_answer_s.append(data_json[str(i+1)]['correct_answer'])\n",
    "                MC_cords_s.append(data_json[str(i+1)]['cords'])\n",
    "            elif data_json[str(i+1)]['type'] == 'TF':\n",
    "                # TF_diagram_path_s.append(int(data_json[str(i+1)]['diagram_path'].split(\"\\\\\")[-1].split(\".\")[0]))\n",
    "                TF_diagram_path_s.append(data_json[str(i+1)]['diagram_path'].split(\"\\\\\")[-1])\n",
    "                TF_question_s.append(data_json[str(i+1)]['question'])\n",
    "\n",
    "                TF_type_s.append(data_json[str(i+1)]['type'])\n",
    "                TF_answer_s.append(data_json[str(i+1)]['answer'])\n",
    "                # TF_correct_answer_s.append('1' if data_json[str(i+1)]['correct_answer']=='a' else '0') \n",
    "                # \"correct_answer\": \"a\"--> \"True\"-->'1',\"correct_answer\": \"b\"--> \"False\"-->'0'\n",
    "                # TF_correct_answer_s.append([1] if data_json[str(i+1)]['correct_answer']=='a' else [0])\n",
    "                TF_correct_answer_s.append(int(1) if data_json[str(i+1)]['correct_answer']=='a' else int(0))\n",
    "                TF_cords_s.append(data_json[str(i+1)]['cords'])\n",
    "            else:\n",
    "                print(\"the file Q_json error\")\n",
    "            \n",
    "    MC = [MC_diagram_path_s, MC_question_s, MC_type_s, MC_answer_s, MC_correct_answer_s, MC_cords_s]\n",
    "    TF = [TF_diagram_path_s, TF_question_s, TF_type_s, TF_answer_s, TF_correct_answer_s, TF_cords_s]\n",
    "    return MC, TF\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4a0576",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(img_data_path):\n",
    "    #img = cv.imread(img_dta_path, cv.IMREAD_COLOR)\n",
    "    print('imgpath', img_data_path)\n",
    "    img = cv.imread(img_data_path)\n",
    "    print(img.shape)\n",
    "    img = np.resize(img, img_size) # h*c\n",
    "    print(img.shape)\n",
    "    # cv.imshow('img', img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4902e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "             nn.Conv2d(in_channels=3,\n",
    "                       out_channels=16,\n",
    "                       kernel_size=3,\n",
    "                       stride=2,\n",
    "                       padding=1),\n",
    "             nn.BatchNorm2d(16),\n",
    "             nn.ReLU(),\n",
    "             )\n",
    "             \n",
    "        self.conv2 = nn.Sequential(\n",
    "             nn.Conv2d(in_channels=16,\n",
    "                       out_channels=1,\n",
    "                       kernel_size=3,\n",
    "                       stride=2,\n",
    "                       padding=1),\n",
    "             nn.BatchNorm2d(1),\n",
    "             nn.ReLU(),\n",
    "             )\n",
    "        \n",
    "        self.Flatten = nn.Flatten()\n",
    "        \n",
    "        self.lin1_img = nn.Linear(8192, 2048)\n",
    "        self.lin2_img = nn.Linear(2048, 1024)\n",
    "        self.lin3_img = nn.Linear(1024, 512)\n",
    "        \n",
    "        self.lin1_x_Q = nn.Linear(2048, 1024)\n",
    "        self.lin2_x_Q = nn.Linear(1024, 512)\n",
    "        \n",
    "        self.lin1_x = nn.Linear(1024, 512)\n",
    "        self.lin2_x = nn.Linear(512, 512)\n",
    "        self.lin3_x = nn.Linear(512, 2)\n",
    "        \n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        #utils.initialize_weight(self)\n",
    "        \n",
    "    def forward(self, x_img, x_Q):\n",
    "        # img feature\n",
    "        x_img = self.conv1(x_img) # 16*256*512-->16*128*256\n",
    "        x_img = self.conv2(x_img) # 16*128*256-->1*64*128-->8192\n",
    "        x_img = self.Flatten(x_img)\n",
    "        x_img = self.lin1_img(x_img)\n",
    "        x_img = self.relu(x_img)\n",
    "        x_img = self.lin2_img(x_img)\n",
    "        x_img = self.relu(x_img)\n",
    "        x_img = self.lin3_img(x_img)\n",
    "        x_img = self.relu(x_img)\n",
    "        x_img = self.lin3_x(x_img)\n",
    "        return x_img\n",
    "    \n",
    "        '''\n",
    "        # word feature\n",
    "        x_Q = self.lin1_x_Q(x_Q)\n",
    "        x_Q = self.relu(x_Q)\n",
    "        x_Q = self.lin2_x_Q(x_Q)\n",
    "        x_Q = self.relu(x_Q)\n",
    "        \n",
    "        # fusion\n",
    "        x = torch.cat((x_img, x_Q), dim=1) \n",
    "        x = self.lin1_x(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.lin2_x(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.lin3_x(x)\n",
    "        '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424dd36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyData(Dataset):\n",
    "    def __init__(self, TF_img1, TF_Q1, labels1):\n",
    "        self.TF_img1 = TF_img1\n",
    "        self.TF_Q1 = TF_Q1\n",
    "        self.labels1 = labels1\n",
    "        \n",
    "        self.transform = torchvision.transforms.Compose([\n",
    "            torchvision.transforms.ToTensor()])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        imag_name = self.TF_img1[idx]\n",
    "        imag_item_path = os.path.join(img_path,imag_name)\n",
    "        img = cv.imread(imag_item_path)\n",
    "        img = cv.resize(img, img_size)\n",
    "        img = self.transform(img)\n",
    "        label = self.labels1[idx]\n",
    "        TF_Q  = self.TF_Q1[idx]\n",
    "        \n",
    "        return img, TF_Q, label # 返回的第item项的图片以及对应的标签\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.TF_img1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340e1cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weight(m):\n",
    "    for m in net.modules():\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            m.weight.data.normal_(0,0.02)\n",
    "            m.bias.data.zero_()\n",
    "        if isinstance(m, nn.BatchNorm2d):\n",
    "            m.weight.data.normal_(0,0.02)\n",
    "            m.bias.data.zero_()\n",
    "        if isinstance(m, nn.Linear):\n",
    "            m.weight.data.normal_(0,0.02)\n",
    "            m.bias.data.zero_()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e883536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18518/513877446.py:79: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target = torch.tensor(target)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss tensor(0.6929, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# TF_data = [TF_diagram_path_s, TF_question_s, TF_type_s, TF_answer_s, TF_correct_answer_s, TF_cords_s]\n",
    "\n",
    "img_path = './data/train/Diagrams/'\n",
    "Q_path = './data/train/Q.json'\n",
    "\n",
    "transf = transforms.ToTensor()\n",
    "\n",
    "img_size = (256, 512)\n",
    "batch_sizes = 20\n",
    "epochs = 20\n",
    "lr = 1e-2\n",
    "\n",
    "net = Net()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "loss = nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "#loss = nn.CrossEntropyLoss()\n",
    "#optimizer = torch.optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# 模型参数加载到新模型\n",
    "model_path = './TF.params'\n",
    "if os.path.exists(model_path):\n",
    "    state_dict=torch.load(model_path)\n",
    "    net.load_state_dict(state_dict)\n",
    "    print('加载...')\n",
    "else:\n",
    "    net.apply(initialize_weight)\n",
    "    print('params...')\n",
    "#print(net)\n",
    "#print(net.state_dict())\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "net = Net().to(device)\n",
    "\n",
    "# read data and shuffle\n",
    "Q_path = './data/train/Q.json'\n",
    "\n",
    "MC_data,  TF_data= read_json(Q_path)\n",
    "#print('TF_data:', len(TF_data), len(TF_data[0]), len(TF_data[5]),len(TF_data[5]))\n",
    "#print('before shuffle TF_data:', TF_data[0][0],TF_data[1][0], TF_data[2][0])\n",
    "\n",
    "\n",
    "for i in range(len(MC_data)):\n",
    "    if len(MC_data[i]) == len(MC_data[0]):\n",
    "        np.random.seed(len(MC_data[0]))\n",
    "        np.random.shuffle(MC_data[i])\n",
    "    else:\n",
    "        print('this MC_data:', i, 'length is error')\n",
    "        \n",
    "for i in range(len(TF_data)):\n",
    "    if len(TF_data[i]) == len(TF_data[0]):\n",
    "        np.random.seed(len(TF_data[0]))\n",
    "        np.random.shuffle(TF_data[i])\n",
    "    else:\n",
    "        print('this TF_data:', i, 'length is error')\n",
    "\n",
    "def train(data_TF):\n",
    "\n",
    "    net.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        for batch_size in range(int(len(TF_data[0])/batch_sizes)): # (998, 998/8=124.75, +1=125,using all data)\n",
    "            \n",
    "            TF_img1 = TF_data[0][batch_size:batch_sizes+batch_size]\n",
    "            # 将这里需要 英语 向量化 如工具word2vec\n",
    "            TF_Q1   = TF_data[1][batch_size:batch_sizes+batch_size]\n",
    "            # TF_Q2cev = \n",
    "            labels1  = TF_data[4][batch_size:batch_sizes+batch_size]\n",
    "\n",
    "            all_data1 = MyData(TF_img1, TF_Q1, labels1)\n",
    "            dataloader_try = DataLoader(all_data1, batch_size=batch_sizes,shuffle=True)\n",
    "            for batch_idx, (img, TF_Q, target) in enumerate(dataloader_try):\n",
    "                #print(batch_idx,len(img))\n",
    "                #print(img.data.size())\n",
    "                \n",
    "                # 读入数据\n",
    "                img = img.to(device)\n",
    "                  \n",
    "                target = torch.tensor(target)\n",
    "                target = target.to(device)\n",
    "                \n",
    "                # 计算模型预测结果和损失\n",
    "                output = net(img, img) # dim is the word2vec\n",
    "                \n",
    "                ls = loss(output, target)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                ls.mean().backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "         \n",
    "        print(epoch, 'loss', ls.mean())\n",
    "        break # train the first batchsize\n",
    "        \n",
    "    torch.save(net.state_dict(), 'TF.params') #只保存加载模型参数\n",
    "      \n",
    "                \n",
    "train(TF_data)           \n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a09e403",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport torch\\nimport torch.nn as nn\\nimport math\\n\\nentroy=nn.CrossEntropyLoss()\\ninput=torch.Tensor([[-0.7715, -0.6205,-0.2562], [-0.7715, -0.6205,-0.2562]])\\ntarget = torch.tensor([0,1])\\nprint('target',target.shape)\\noutput = entroy(input, target)\\nprint(output)\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "entroy=nn.CrossEntropyLoss()\n",
    "input=torch.Tensor([[-0.7715, -0.6205,-0.2562], [-0.7715, -0.6205,-0.2562]])\n",
    "target = torch.tensor([0,1])\n",
    "print('target',target.shape)\n",
    "output = entroy(input, target)\n",
    "print(output)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5388df4120428dd47153b181c178739ab52ad4b02e3a838223d2b0ec429ae0a3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
